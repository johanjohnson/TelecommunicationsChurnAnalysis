{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span>PREDICTIVE ANALYSIS</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Question**\n",
    "Which clients are most likely to abandon the telecommunications firm, and why? What characteristics are particularly significant in terms of reducing customer churn? The predictive method that will be utilized is decision trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Goal**\n",
    "Organizational stakeholders will benefit from knowing, with some confidence, which consumers are more likely to churn and which are not. As a result of statistical analysis, stakeholders will be able to acquire answers to their questions and make better business decisions. Understanding the churn rate will also help you sell better services to customers and understand previous user experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Justification for Decision Tree Method**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classification and regression tree is made up of a sequence of if/else queries about specific categorical variables that are used to infer labels. Without feature scaling, the classification tree can capture non-linear connections between features and labels. Every branch represents an attribute, and at the conclusion of each node, the decision tree will determine the optimal decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Assumptions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, the entire training set is regarded as the root. Categorical feature values are desired. If the values are continuous, they must be discretized before the model can be built. On the basis of attribute values, records are dispersed iteratively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prepared Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "newchurns = pd.read_csv('DecisionTreeClassfication.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 35)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newchurns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predictor Variables will be stored into X. Y will have the values of EncodedChurn. \n",
    "x = newchurns.drop('EncodedChurn', axis=1).values\n",
    "y = newchurns['EncodedChurn'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Scikit-learn Libraries\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error as MSE\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Splitting the data into training and test data sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.3, stratify= y, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Analysis Technique and Intermediate Calculations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "#Instantiate dt \n",
    "dt = DecisionTreeClassifier(max_depth = None, random_state = 1)\n",
    "#Fit dt to training set \n",
    "dt.fit(x_train, y_train)\n",
    "#predict test set labels \n",
    "y_prediction = dt.predict(x_test)\n",
    "print(y_prediction[0:5])\n",
    "#accuracy_score \n",
    "acc_score = accuracy_score(y_test, y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score 0.797\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy score {:.3f}\".format(acc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1872  333]\n",
      " [ 275  520]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86      2205\n",
      "           1       0.61      0.65      0.63       795\n",
      "\n",
      "    accuracy                           0.80      3000\n",
      "   macro avg       0.74      0.75      0.75      3000\n",
      "weighted avg       0.80      0.80      0.80      3000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Prediction Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_impurity_split': None, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'random_state': 1, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "#Inspecting the hyperparameters of a CART in sk-learn\n",
    "print(dt.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\miniconda3\\envs\\ds\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.735 0.735 0.735 ...   nan   nan   nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, estimator=DecisionTreeClassifier(random_state=1), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini', 'entropy'],\n",
       "                         'max_depth': array([1, 2, 3, 4, 5, 6, 7, 8, 9]),\n",
       "                         'max_features': [0.2, 0.4, 0.6, 0.8, 0.1, 0.12, 0.16,\n",
       "                                          0.18],\n",
       "                         'min_samples_leaf': array([0.01, 0.11, 0.21, 0.31, 0.41, 0.51, 0.61, 0.71, 0.81, 0.91])},\n",
       "             scoring='accuracy')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GridSearchCV\n",
    "params_dt = { \n",
    "            'max_depth':np.arange(start=1, stop=10, step=1),\n",
    "            'min_samples_leaf': np.arange(start=0.01, stop=1, step=0.10),\n",
    "            'max_features': [0.2, 0.4,0.6,0.8,0.10,0.12,0.16,0.18],\n",
    "            'criterion':['gini','entropy']\n",
    "            }\n",
    "grid_dt = GridSearchCV(estimator = dt, param_grid = params_dt, scoring = 'accuracy', cv = 10, n_jobs = -1)\n",
    "grid_dt.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 9, 'max_features': 0.8, 'min_samples_leaf': 0.01}\n"
     ]
    }
   ],
   "source": [
    "best_hyperparams = grid_dt.best_params_\n",
    "print(best_hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_CV_score = grid_dt.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the best model: 0.847\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of the best model: {:.3f}\".format(best_CV_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE of decision tree model for testing data:0.104\n",
      "RMSE score Decison Tree Regressor model for testing data:0.323\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeRegressor(max_depth = 9, min_samples_leaf = 0.01, random_state = 1)\n",
    "dt.fit(x_train, y_train)\n",
    "y_pred = dt.predict(x_test)\n",
    "mse_dt = MSE(y_test, y_pred)\n",
    "rmse_dt = mse_dt ** (1/2)\n",
    "print(\"MSE of decision tree model for testing data:{:.3f}\".format(mse_dt))\n",
    "print(\"RMSE score Decison Tree Regressor model for testing data:{:.3f}\".format(rmse_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "top_model = grid_dt.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = top_model.predict_proba(x_test)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc = roc_auc_score(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC Score 0.909\n"
     ]
    }
   ],
   "source": [
    "print(\"ROC AUC Score {:.3f}\".format(roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.90657402 0.89602683 0.91439909 0.90108034 0.90285984]\n"
     ]
    }
   ],
   "source": [
    "#AUC Using cross_validation \n",
    "from sklearn.model_selection import cross_val_score \n",
    "cross_val_scores = cross_val_score(dt, x_train, y_train, cv = 5, scoring = 'roc_auc')\n",
    "print(cross_val_scores) #AUC scores computed using 5-fold cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explanation for the accuracy score and the Mean Squared Error**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy score for the original model was reported to be 0.797. However, after hyperparameter adjustment, the best model's accuracy score was increased to.847. GridSearchCV was used to select the best hyperparameters. Entropy was the best criterion, with an optimal max depth of nine, max features of 0.80, and a min sample leaf of 0.0100. The AOC score for ROU was also reported to be.909.\n",
    "The MSE (Mean Squared Error), which is calculated by squaring the average difference over the data set, represents the difference between actual and projected values. It's a measure of how close a fitted line is to actual data points. RMSE is the error rate multiplied by the square root of MSE (Root Mean Squared Error).\n",
    "For testing data, the MSE of the decision tree model is 0.104, while the RMSE score for the decision tree regressor model is 0.323.\n",
    "The MSE and RMSE are lower the closer the fit is to the data set.\n",
    "With a low mean squared error, we have a good model with high prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Limitation of analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the decision trees' drawbacks is that they are very unstable when compared to other choice predictors. A slight change in the data might cause a significant structural transformation of the tree structure, resulting in an outcome that differs from what consumers would expect in a typical event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Recommendation**\n",
    "The organization can use this decision tree algorithm to anticipate the customer's tenure based on the analysis done and the model's accuracy score. Additional parameter adjustments on additional parameters and the use of more labeled data to train the model can improve the model even further.\n",
    "It's also vital for decision-makers to know that the predictor factors that were analyzed produce a reasonable accuracy score. Thus, we should look at the characteristics that are common among individuals who are leaving the organization and try to reduce the possibility that they will occur with any specific customer in the future. This implies that as a client signs up to more than just the company's services, they are less likely to depart."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c1afc5aea9ff721e5149f41f35b12d8c2e3573b8faf0316c405784c06165b753"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('minimal_datascience': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
